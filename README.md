# polaris

The Golang version of the databuilder execution framework - https://github.com/flipkart-incubator/databuilderframework

The below content is a modification of <a href="https://github.com/flipkart-incubator/databuilderframework/blob/master/README.md">this README</a>, based on the changes of implementing this framework in golang.

## Introduction

The polaris framework is a high level logic execution engine that can be used to execute multi-step workflows. You should look at this framework for the following scenarios:

1. Multi-step work flow executions where each step is dependent on data generated from previous steps
2. Executions can span one request scope or multiple scopes.
3. Your systems works with reusable components that can be combined in different ways to generate different end-results

## Terminologies

We've aimed to make minimal changes to terminologies as they were in the original Java framework.

* _**Data**_ - The basic container for information generated by an actor in the system. Meta associated:
    * **Data** - Name of the data
* _**Builder**_ - An actor that consumes a bunch of data and produces another data. It has the following meta associated with it:
    * **Name** - Name of the builder
    * **Consumes** - A set of Data that the builder consumes
    * **Prodcues** - Data that the builder produces
* _**Workflow**_ - A specification and container for a topology of connected builders that generate a final data. It has the following meta:
    * **Name** - Name of the workflow
    * **Target Data** - The name of the data being generated by this data flow
    * **Resolution Specs** - If multiple builders known to the system can produce the same data, then this can be used to put an override specifying which particular builder will generate a particular data in context of this data flow.
    * **Transients** - A set of names of Data that would be considered Transients for this case. (See later for a detailed explanation of transients)
* _**ExecutionGraph**_ - A graph of connected and topologically sorted builders that are used by the execution engine to execute a flow. 
* _**DataSet**_ - A set of the Data provided by the client and generated internally by the different builders of a particular ExecutionGraph
* _**DataFlowInstance**_ - An instantiation of DataFlow that contains it's own copy of ExecutionGraph and DataSet. This represents the execution context of a particular request.
* _**DataDelta**_ - The set of new data that needs to be considered as input for a particular execution
* _**DataFlowExecutor**_ - The core engine that uses the provided DataDelta to execute the ExecutionGraph present in the given DataFlowInstance. This will augment the DataSet within the DataFlowInstance with the non-transient Data generated by the different Builders. All newly generated by the engine (including generated transient data) is returned by the engine.
* _**DataSetAccessor**_ - A typesafe utility for accesing data present in a DataSet. This is used inside the builders to generate data.


## Features
1. Listeners for pre-execution, post-execution and post-error for builders as well as workflows
2. Recon framework that will traverse the graph perform a series of actions that will re-run the stuck workflow
3. Events framework that uses listeners to push events for builder as well as workflow execution/errors

## Limitations
I'm still trying to figure out how to implement versioning for workflows.